# ğŸ™ï¸ Speech Emotion Recognition using Librosa and Machine Learning

This project focuses on building a Speech Emotion Recognition (SER) system that classifies audio clips into different emotional states such as **happy**, **sad**, **angry**, and **neutral** using extracted features like MFCC, Chroma, and Mel spectrograms.

---

## ğŸ“‚ Dataset
- **Source**: RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)
- **Classes**: Neutral, Calm, Happy, Sad, Angry, Fearful, Disgust, Surprised
- **Format**: `.wav` files

---

## ğŸ› ï¸ Technologies Used
- **Python**
- **Jupyter Notebook**
- **Librosa** â€“ for audio processing
- **SoundFile** â€“ to read audio data
- **Scikit-learn** â€“ model building, evaluation, and hyperparameter tuning
- **Matplotlib / Seaborn** â€“ visualizations

---

## ğŸ§  Features Extracted
- **MFCC (Mel-Frequency Cepstral Coefficients)**
- **Chroma Features**
- **Mel Spectrogram**

These features were concatenated to form a numerical feature vector for each audio file.

---

## ğŸš€ Model & Performance
- **Classifier Used**: `MLPClassifier` (Multi-layer Perceptron)
- **Accuracy Achieved**: ~55.67%
- **Evaluation Metrics**: Accuracy, Confusion Matrix, Classification Report

---

## ğŸ” Model Evaluation

```python
from sklearn.metrics import confusion_matrix, classification_report
